{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad23322",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Financial News Headlines with Market Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4a908",
   "metadata": {},
   "source": [
    "## Cardiff University School of Computer Science and Informatics - Final Year Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19f242a",
   "metadata": {},
   "source": [
    "Growth of the internet and the digital economy, along with technical advances in computer and data science have supported a wave of alternative data sources that can be used to measure and predict the financial markets. One of these non-traditional metrics is public opinion mining, commonly referred to as sentiment analysis. This study investigates the hypothesis that the sentiment of financial news headlines reflects and directs the performance of the U.S. stock market through proving a significant correlation between the polarity of the sentiment and the change in price of a security, thus working to disprove the controversial ‘efficient market hypothesis’. To evaluate the publics sentiment a vast dataset of ‘financial news’ headlines are required ranging over a broad period. Additionally, a natural language processing and machine learning classification model is built to predict the sentiment polarity of headlines. Finally, statistical analysis is conducted on the data to prove any significant correlation within the results. The study can demonstrate the hypothesis to an extent, showing that the sentiment of financial news headlines relating to the overall U.S. market, directly reflects the price of the U.S. market index. Despite this no correlation between the price of an individual stock and the sentiment of directly relating financial news headlines could be found. Additionally, there was no evidence to suggest that the daily sentiment of a security had any influence over its corresponding price change the subsequent day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collect Headline Data\"\"\"\n",
    "\n",
    "import pandas\n",
    "\n",
    "cnbcData = pandas.read_csv('Data/Original/cnbc_headlines.csv')\n",
    "guardianData = pandas.read_csv('Data/Original/guardian_headlines.csv')\n",
    "reutersData = pandas.read_csv('Data/Original/reuters_headlines.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d41e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Collect and Clean Market Data\"\"\"\n",
    "\n",
    "import yfinance\n",
    "\n",
    "def getMarketData(ticker):\n",
    "    \"\"\"\n",
    "    Takes a stock market ticker (string) as a parameter and returns the close prices\n",
    "    of that stock (list of float values) each trading day between 2017-12-22 and 2020-07-18.\n",
    "    \n",
    "    EXAMPLE: '^GSPC' -> [3962.18, 3965.72, 413.21, ...]\n",
    "    \n",
    "    ARGS:\n",
    "        ticker (string) - A stock market ticker value. \n",
    "    \n",
    "    RETURNS:\n",
    "        securityData (list of float values) - - A list of stock close price values between 2017-12-22 and 2020-07-18.\n",
    "    \"\"\"\n",
    "    securityData = yfinance.download(ticker, start='2017-12-22', end='2020-07-18')\n",
    "    securityData = securityData['Close']\n",
    "    return securityData\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf56c5",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f534c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Initial Clean Headline Data\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def cleanData(dataSet):\n",
    "    \"\"\"\n",
    "    Takes a news headline pandas dataframe as a parameter and removes any irrelevant\n",
    "    data (for the scope of the project) that may be present, returning the cleaned data frame.\n",
    "    \n",
    "    ARGS:\n",
    "        dataSet (pandas data frame) - Financial news headline data frame containing irrelevent data columns. \n",
    "    \n",
    "    RETURNS:\n",
    "        dataSet (pandas data frame) - Cleaned financial news headline data frame.\n",
    "    \"\"\"\n",
    "    dataSet = dataSet.dropna()\n",
    "    try:\n",
    "        dataSet = dataSet.drop('Description', axis=1)\n",
    "    except:\n",
    "        print(\"Headline data set contains no description\")\n",
    "    dataSet = dataSet.drop_duplicates(subset=['Headlines'], keep='first')\n",
    "    dataSet.reset_index(drop=True, inplace=True)\n",
    "    return dataSet\n",
    "\n",
    "def dateConversion(date):\n",
    "    \"\"\"\n",
    "    Takes date values found within the CNBC data set as parameters and returns them converted to a format\n",
    "    that can be manipulated to generate 'datetime64' values.\n",
    "    \n",
    "    ARGS: \n",
    "        date (string) - An incorrectly fromated string data value. \n",
    "    \n",
    "    RETURNS:\n",
    "        date (string) - A correctly fromated string data value\n",
    "    \"\"\"\n",
    "    date = date.replace(\"Sept\", \"Sep\").replace(\"March\", \"Mar\").replace(\"April\", \"Apr\").replace(\"June\", \"Jun\").replace(\"July\", \"Jul\")\n",
    "    if date[0].isspace():\n",
    "        date = date.replace(\" \", \"0\", 1)\n",
    "        date = date.replace(\",  \", \", 0\", 1)\n",
    "    return date\n",
    "\n",
    "#CNBC\n",
    "\n",
    "cnbcData = cleanData(cnbcData)\n",
    "dateFormat = '%I:%M  %p ET %a, %d %b %Y'\n",
    "dates = []\n",
    "for item in cnbcData.iloc[:, 1].values:\n",
    "    item = dateConversion(item)\n",
    "    dates.append(datetime.strptime(item, dateFormat).strftime(\"%m-%d-%Y\"))\n",
    "cnbcData[\"Time\"] = dates\n",
    "cnbcData[\"Time\"] = cnbcData[\"Time\"].astype(\"datetime64\")\n",
    "cnbcData.rename(columns={\"Time\":\"Date\"}, inplace = True)\n",
    "\n",
    "#Guardian\n",
    "\n",
    "guardianData = cleanData(guardianData)\n",
    "guardianData[\"Time\"] = pandas.to_datetime(guardianData[\"Time\"], errors = 'coerce', format=\"%d-%b-%y\")\n",
    "guardianData.rename(columns={\"Time\":\"Date\"}, inplace = True)\n",
    "\n",
    "#Reuters\n",
    "\n",
    "reutersData = cleanData(reutersData)\n",
    "reutersData[\"Time\"] = reutersData[\"Time\"].astype(\"datetime64\")\n",
    "reutersData.rename(columns={\"Time\":\"Date\"}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6f286b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Combine Headline Data\"\"\"\n",
    "\n",
    "dataSets = [cnbcData, guardianData, reutersData]\n",
    "headlineData = pandas.concat(dataSets)\n",
    "print(\"Pre Cleaning: \")\n",
    "headlineData.info()\n",
    "headlineData = headlineData.sort_values(by=\"Date\")\n",
    "headlineData = cleanData(headlineData)\n",
    "print(\" \")\n",
    "print(\"Post Cleaning: \")\n",
    "headlineData.info()\n",
    "headlineData.to_csv('Data/all_headlines.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cb2e7d",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06e125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Explority Data Analysis - Investigating Distribution\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 2))\n",
    "\n",
    "#Time Series Plot\n",
    "spy = getMarketData('SPY')\n",
    "axs[0].plot(spy, 'tab:red')\n",
    "axs[0].set_title('S&P 500 Index Time Series')\n",
    "axs[0].axes.get_xaxis().set_ticks([])\n",
    "axs[0].set_xlabel('Date (December 2017 - July 2020)')\n",
    "axs[0].set_ylabel('Price (USD)')\n",
    "\n",
    "#Frequency Distribution Plot\n",
    "headlineQuantity = headlineData[\"Headlines\"].groupby([headlineData[\"Date\"].dt.year, headlineData[\"Date\"].dt.month]).count().tolist()\n",
    "axs[1].bar(list(range(len(headlineQuantity))), headlineQuantity)\n",
    "axs[1].set_title('Number Of Headlines Published Monthly')\n",
    "axs[1].axes.get_xaxis().set_ticks([])\n",
    "axs[1].set_xlabel('Month (December 2017 - July 2020)')\n",
    "axs[1].set_ylabel('Headlines')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e48bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Explority Data Analysis - Vocabulary Representation\"\"\"\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(17, 6))\n",
    "\n",
    "reutersDataString = \" \".join(reutersData[\"Headlines\"].to_list())\n",
    "guardianDataString = \" \".join(guardianData[\"Headlines\"].to_list())\n",
    "cnbcDataString = \" \".join(cnbcData[\"Headlines\"].to_list())\n",
    "#Generating Word Clouds For Each Dataset\n",
    "reutersWordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', max_words=250).generate(reutersDataString)\n",
    "guardianWordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', max_words=250).generate(guardianDataString)\n",
    "cnbcWordcloud = WordCloud(stopwords=STOPWORDS, background_color='black', max_words=250).generate(cnbcDataString)\n",
    "\n",
    "#Word Cloud Plot\n",
    "axs[0].imshow(reutersWordcloud, interpolation='bilinear')\n",
    "axs[0].set_title('Reuters Word Cloud')\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].imshow(guardianWordcloud, interpolation='bilinear')\n",
    "axs[1].set_title('Guardian Word Cloud')\n",
    "axs[1].axis(\"off\")\n",
    "axs[2].imshow(cnbcWordcloud, interpolation='bilinear')\n",
    "axs[2].set_title('CNBC Word Cloud')\n",
    "axs[2].axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3dc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Explority Data Analysis - Word Type Frequency\"\"\"\n",
    "\n",
    "import nltk, nltk.classify\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "typeList = []\n",
    "#Basic Pre-Processing\n",
    "headlineString = \" \".join(headlineData[\"Headlines\"])\n",
    "headlineString = re.sub(r'[^a-zA-Z]', \" \", headlineString.lower())\n",
    "tokenizedHeadlines = nltk.word_tokenize(str(headlineString))\n",
    "wordTypeList = nltk.pos_tag(tokenizedHeadlines) #Word Type Identfication\n",
    "for wordType in wordTypeList:\n",
    "    typeList.append(wordType[1])\n",
    "wordsTypeFrequency = Counter(typeList) #Count Word Type Frequency\n",
    "wordsTypeFrequency = {k: v for k, v in sorted(wordsTypeFrequency.items(), key=lambda item: item[1])}\n",
    "\n",
    "label = list(wordsTypeFrequency.keys())\n",
    "frequency = list(wordsTypeFrequency.values())\n",
    "    \n",
    "#Word Type Frequency Plot\n",
    "plt.figure(figsize=(16, 2))\n",
    "plt.bar(label, frequency)\n",
    "plt.xlabel('Word Type')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Word Type And Respective Frequency Of Occurrence In All Headlines')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433aa339",
   "metadata": {},
   "source": [
    "## Data Pre-Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb2c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Pre-Processing\"\"\"\n",
    "\n",
    "from nltk.corpus import words, stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "#nltk.download()\n",
    "\n",
    "def preProcessing(text):\n",
    "    \"\"\"\n",
    "    Processes a raw text string (parameter) through various NLP pre-processing techniques\n",
    "    and returns a reduced list of words suitable for producing a BoW model.\n",
    "            \n",
    "    EXAMPLE: '#024: Only the best stock can survive in this economy!' -> ['only', 'best', 'stock', 'survive', 'economy']\n",
    "    \n",
    "    ARGS:\n",
    "        text (string) - The raw text headline under consideration.\n",
    "    \n",
    "    RETURNS:\n",
    "        text (list of string values) - A list of words that passed the pre-processing criteria from the string passed in.\n",
    "    \"\"\"\n",
    "    text = namedEntityRecognition(text)[0] #Named Entity Recognition\n",
    "    text = re.sub(r'[^a-zA-Z]', \" \", text.lower()).split() #Formatting Words\n",
    "    text = list(set(text) - set(str(stopwords))) #Stop Word Removal\n",
    "    text = list(set(text) & set(words.words())) #Non-Word Removal \n",
    "    for word in text:\n",
    "        word = lemmatizer.lemmatize(word) #Lemmatization\n",
    "        word = stemmer.stem(str(word)) #Stemming \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580dcc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Named Entity Recognition\"\"\"\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def namedEntityRecognition(text):\n",
    "    \"\"\"\n",
    "    Locates and classifies specific named entities within unstructured text (parameter) and classifies\n",
    "    them into pre-defined categories. In this custom function, company names being considered within the \n",
    "    scope of the problem are identified, removed, and stored.\n",
    "            \n",
    "    EXAMPLE: 'This is why I hate Walmart and Amazon!' -> 'This is why I hate COMPANY and COMPANY', ['WMT', 'AMZN']\n",
    "    \n",
    "    ARGS:\n",
    "        text (string) - The raw text headline under consideration.\n",
    "    \n",
    "    RETURNS:\n",
    "        text (string) - The raw text headline under consideration with named entities removed.\n",
    "        securities (list of string values) - A list of the securities ticker strings identified within the headline. \n",
    "    \"\"\"\n",
    "    selectedSecurities = {'S&P': 'SPY', 'google': 'GOOGL', 'amazon': 'AMZN', 'apple': 'AAPL',\n",
    "                          'microsoft': 'MSFT','visa': 'V', 'johnson': 'JNJ', 'walmart': 'WMT',\n",
    "                          'exxon': 'XOM', 'FB': 'facebook', 'TSLA': 'tesla'}\n",
    "    securityNames = selectedSecurities.keys()\n",
    "    text = text.lower().split()\n",
    "    for word in text:\n",
    "        word = stemmer.stem(str(word)) #Reduce Company To Stem\n",
    "    securities = []\n",
    "    for security in securityNames:\n",
    "        if security in text:\n",
    "            text[text.index(security)] = 'COMPANY' #Company Name Removal\n",
    "            securities.append(selectedSecurities[security]) #Identify for Analysis \n",
    "    if len(securities) == 0:\n",
    "        securities.append('^GSPC')\n",
    "    text = ' '.join(text)\n",
    "    text = NER(text)\n",
    "    text = ' '.join([t.text if not t.ent_type_ else t.ent_type_ for t in text]).lower()\n",
    "    return text, securities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3eadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generate Word Set (Bag-Of-Words Representation)\"\"\"\n",
    "\n",
    "import numpy \n",
    "import time\n",
    "\n",
    "wordSet = []\n",
    "start = time.time()\n",
    "for headline in headlineData[\"Headlines\"].to_list():\n",
    "    composedHeadline = preProcessing(headline)\n",
    "    wordSet = numpy.union1d(wordSet, composedHeadline) #Add Word If Not Present In BoW\n",
    "end = time.time()\n",
    "textfile = open(\"Data/wordset.txt\", \"w\")\n",
    "for element in wordSet:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()\n",
    "\n",
    "print(\"Runtime: \" + str(round((end - start), 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c1c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"Dimensionality Reduction\"\"\"\n",
    "\n",
    "headlineString = \" \".join(headlineData[\"Headlines\"])\n",
    "headlineString = re.sub(r'[^a-zA-Z]', \" \", headlineString.lower()).split()\n",
    "print('Pre Pre-Processing Dimensionality: ' + str(len(set(headlineString))))\n",
    "\n",
    "wordList = open('Data/wordset.txt', 'r')\n",
    "wordSet = wordList.read().split()\n",
    "print('Post Pre-Processing Dimensionality: ' + str(len(wordSet)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b91bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feature Extraction\"\"\"\n",
    "\n",
    "def featureExtraction(words, document):\n",
    "    \"\"\"\n",
    "    Extracts feature values from a document by counting their occurrence, utilisng the BoW \n",
    "    model, and generating and returning a frequency dictionary.\n",
    "            \n",
    "    EXAMPLE: ['microsoft', 'a', 'employee', 'awful', 'finance'], 'A Microsoft employee provides awful tales of neglect' \n",
    "    -> {'microsoft': 1, 'a': 1, 'employee': 1, 'awful': 1, 'finance': 0}\n",
    "    \n",
    "    ARGS:\n",
    "        words (list of string values) - Words contained within the BoW model.\n",
    "        document (string) - The headline document under consideration.\n",
    "    \n",
    "    RETURNS:\n",
    "        wordFrequency (dictionary) - The words and their corrasponding frequency of occurance in the headline.\n",
    "    \"\"\"\n",
    "    wordFrequency = dict.fromkeys(words, 0)\n",
    "    for word in document:\n",
    "        if word in words:\n",
    "            wordFrequency[word] = document.count(word) #Frequency Of Occurrence\n",
    "    return wordFrequency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec0909",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Annotating Training Data\"\"\"\n",
    "\n",
    "import random\n",
    "\n",
    "#Data Acsess\n",
    "wordList = open('Data/wordset.txt', 'r')\n",
    "wordSet = wordList.read().split()\n",
    "headlineData = pandas.read_csv('Data/all_headlines.csv')\n",
    "\n",
    "#Data Frame Construction\n",
    "annotatedTrainingData = pandas.DataFrame(data={'Document': [], 'Sentiment': []})\n",
    "trainingFeatures = pandas.DataFrame(data=dict.fromkeys(wordSet, []))\n",
    "annotatedTrainingData = annotatedTrainingData.append(trainingFeatures, ignore_index=True)\n",
    "#trainingDataCorpus = pandas.read_csv('Data/training_data.csv')\n",
    "\n",
    "def annotateTrainingData():\n",
    "    \"\"\"\n",
    "    This method allows a user to manually annotate randomly selected headlines from a dataset with their\n",
    "    corrasponding sentiment. This is utilised to construct a test-train data set as a .csv file.\n",
    "            \n",
    "    EXAMPLE:\n",
    "    \n",
    "    >>> 'This is why I love Google's financial strategy' - Enter the sentiment of this headline:\n",
    "    >>>\n",
    "    \n",
    "    \"\"\"\n",
    "    #Instructions\n",
    "    print(\"Evaluate the overall sentiment of each headline:\")\n",
    "    print(\"If positive enter 'positive', if negative enter 'negative' and if nuetral enter 'nuetral'.\")\n",
    "    print(\"Enter 'END' at any point to stop the process!\")\n",
    "    unprocessedHeadlines = headlineData[\"Headlines\"].to_list()\n",
    "    for headline in unprocessedHeadlines:\n",
    "        headline = namedEntityRecognition(headline)[0]\n",
    "    entry = \"\"\n",
    "    while entry != \"END\":\n",
    "        headlineIndex = random.randint(0, len(unprocessedHeadlines)) #Random headline Selection\n",
    "        headline = unprocessedHeadlines[headlineIndex]\n",
    "        unprocessedHeadlines.remove(headline)\n",
    "        entry = input(headline + \" - Enter the sentiment of this headline: \") #Labeling\n",
    "        if entry != 'END':\n",
    "            #Process Dataset For Machine Learning\n",
    "            sentiment = entry\n",
    "            features = list(featureExtraction(wordSet, preProcessing(headline)).values())\n",
    "            entry = [headline, sentiment]\n",
    "            for feature in features:\n",
    "                entry.append(feature)\n",
    "            #Add Entry To Data Frame\n",
    "            annotatedTrainingData.loc[len(annotatedTrainingData)] = entry\n",
    "            annotatedTrainingData.to_csv('Data/Training/annotated_training_data.csv')\n",
    "            headlineData.drop(headlineIndex, axis=0)\n",
    "            headlineData.to_csv('Data/all_headlines.csv')\n",
    "\n",
    "#annotateTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"External Training Data (Document-Term Matrix)\"\"\"\n",
    "\n",
    "#Data Acsess\n",
    "externalData = pandas.read_csv('Data/Training/external_data.csv', encoding = 'unicode_escape', engine ='python')\n",
    "externalData = cleanData(externalData)\n",
    "wordList = open('Data/wordset.txt', 'r')\n",
    "wordSet = wordList.read().split()\n",
    "\n",
    "#Data Frame Construction\n",
    "externalTrainingData = pandas.DataFrame(data={'Document': [], 'Sentiment': []})\n",
    "trainingFeatures = pandas.DataFrame(data=dict.fromkeys(wordSet, []))\n",
    "externalTrainingData = externalTrainingData.append(trainingFeatures, ignore_index=True)\n",
    "\n",
    "headlines = externalData[\"Headlines\"].to_list()\n",
    "sentiment = externalData[\"Sentiment\"].to_list()\n",
    "\n",
    "#Process Dataset For Machine Learning\n",
    "for row in range(1, len(headlines)):\n",
    "    headlines[row] = namedEntityRecognition(headlines[row])[0] \n",
    "    features = list(featureExtraction(wordSet, preProcessing(headlines[row])).values())\n",
    "    entry = [headlines[row], sentiment[row]]\n",
    "    for feature in features:\n",
    "        entry.append(feature)\n",
    "    #Add Entry To Data Frame\n",
    "    externalTrainingData.loc[len(externalTrainingData)] = entry\n",
    "externalTrainingData.to_csv('Data/Training/external_training_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2096f6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Combine Training Data\"\"\" #Redundant With No Annotation \n",
    "\n",
    "annotatedTrainingData = pandas.read_csv('Data/Training/annotated_training_data.csv')\n",
    "externalTrainingData = pandas.read_csv('Data/Training/external_training_data.csv')\n",
    "\n",
    "labledData = pandas.concat([annotatedTrainingData, externalTrainingData])\n",
    "labledData = labledData.sample(frac = 1) #Shuffle Data\n",
    "labledData.to_csv('Data/Training/all_labled_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feeding\"\"\" \n",
    "\n",
    "labledData = pandas.read_csv('Data/Training/all_labled_data.csv')\n",
    "\n",
    "#Classifier Training Format\n",
    "structuredLabledData = []\n",
    "for index, row in labledData.iterrows():\n",
    "    features = {}\n",
    "    sentiment = row['Sentiment']\n",
    "    for feature in wordSet:\n",
    "        features[feature] = row[feature]\n",
    "    structuredLabledData.append((features, sentiment))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abc54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Test-Train Split\"\"\"\n",
    "\n",
    "#80-20 Split\n",
    "onePercent = len(structuredLabledData)//100 \n",
    "trainingData, testingData = structuredLabledData[onePercent*80:], structuredLabledData[:onePercent*20]\n",
    "xTrain, yTrain, xTest, yTest = [], [], [], []\n",
    "for observation in trainingData:\n",
    "    xTrain.append(observation[0])\n",
    "    yTrain.append(observation[1])\n",
    "for observation in testingData:\n",
    "    xTest.append(observation[0])\n",
    "    yTest.append(observation[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7da40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Standardisation\"\"\"\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "numericFeatureList = []\n",
    "for values in xTrain:\n",
    "    numericFeatures = list(xTrain[0].values())\n",
    "    numericFeatureList.append(numericFeatures)\n",
    "\n",
    "#Standardize Data \n",
    "scaler.fit(numericFeatureList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3235b8",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Classifier Testing\"\"\"\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, ConfusionMatrixDisplay\n",
    "\n",
    "def evaluationMeasures(classifier, testData):\n",
    "    \"\"\"\n",
    "    Takes a classification model and testing data as parameters and produces a series of\n",
    "    evaluation metrics and plots to determine the ability of the model.\n",
    "    \n",
    "    ARGS:\n",
    "        classifier (function) - Machine learning classification model.\n",
    "        testData (data frame array) - Headlines and their corresponding sentiment. \n",
    "    \n",
    "    EXAMPLE:\n",
    "        \n",
    "        Accuracy: 0.81\n",
    "        Recall: 0.74\n",
    "        Precision: 0.62\n",
    "        F1 Score: 0.77\n",
    "        \n",
    "    \"\"\"\n",
    "    #Confusion Matrix\n",
    "    testFeatures = [feature for (feature, label) in testData]\n",
    "    testLabels = [label for (feature, label) in testData]\n",
    "    testPredicted = [classifier.classify(feature) for feature in testFeatures]\n",
    "    confusionMatrix = sklearn.metrics.confusion_matrix(testLabels, testPredicted)\n",
    "    display = ConfusionMatrixDisplay(confusion_matrix = confusionMatrix)\n",
    "    #Alternate Metrics\n",
    "    accuracy = accuracy_score(testPredicted, testLabels) #Accuracy\n",
    "    recall = recall_score(testPredicted, testLabels, average=None) #Recall\n",
    "    precision = precision_score(testPredicted, testLabels, average=None) #Precision\n",
    "    f1Score = f1_score(testPredicted, testLabels, average=None) #F1 Score\n",
    "    #Output\n",
    "    display.plot()\n",
    "    print(\" \")\n",
    "    print(\"Accuracy: \" + str(round(accuracy, 2)))\n",
    "    print(\"Recall: \" + str(round(recall[0], 2)))\n",
    "    print(\"Precision: \" + str(round(precision[0], 2)))\n",
    "    print(\"F1 Score: \" + str(round(f1Score[0], 2)))\n",
    "    print(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb6962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing Baseline\"\"\"\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "baselineClassifier = DummyClassifier(strategy=\"stratified\")\n",
    "baselineClassifier.fit(xTrain, yTrain)\n",
    "print(\" \")\n",
    "print(\"Stratified Classification Baseline: \")\n",
    "print(\"Accuracy: \" + str(round(baselineClassifier.score(xTest, yTest), 2)))\n",
    "print(\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a086ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Naive Bayes Evaluation (No Cross Validation)\"\"\"\n",
    "\n",
    "nbClassifier = nltk.NaiveBayesClassifier.train(trainingData)\n",
    "print(\"Naive Bayes Classifier: \")\n",
    "evaluationMeasures(nbClassifier, testingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19c136",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Informative Features\"\"\"\n",
    "\n",
    "nbClassifier.show_most_informative_features(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce82bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cross Validation\"\"\"\n",
    "\n",
    "import sklearn.model_selection\n",
    "\n",
    "def crossValidation(classifier, trainingData):\n",
    "    \"\"\"\n",
    "    Performs cross validation on a classifier and training data (parameters) and returns the trained classifier.\n",
    "    \n",
    "    ARGS:\n",
    "        classifier (function) - Machine learning classification model.\n",
    "        trainingData (data frame array) - Headlines and their corresponding sentiment. \n",
    "    \n",
    "    RETURNS:\n",
    "        classifier (function) - Trained machine learning classification model.\n",
    "    \"\"\"\n",
    "    KFoldCV = sklearn.model_selection.KFold(n_splits=10) #10 Folds\n",
    "    KFoldAccuracy = []\n",
    "    split = 1\n",
    "    for trainIndex, testIndex in KFoldCV.split(trainingData): #Fold Testing\n",
    "        classifier = classifier.train(trainingData[trainIndex[0]:trainIndex[len(trainIndex) - 1]])\n",
    "        split += 1\n",
    "    return classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f10342",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cross Validation Impact\"\"\"\n",
    "\n",
    "print(\"Evaluation Measures After Cross Validation (Naive Bayes): \")\n",
    "nbClassifier = crossValidation(nbClassifier, structuredLabledData)\n",
    "print(\" \")\n",
    "evaluationMeasures(nbClassifier, testingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bca6957",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Support Vector Machine Evaluation\"\"\"\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "svmClassifier = nltk.classify.SklearnClassifier(SVC())\n",
    "svmClassifier = crossValidation(svmClassifier, structuredLabledData)\n",
    "print(\"Support Vector Machine Classifier: \")\n",
    "evaluationMeasures(svmClassifier, testingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982bdd00",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"Logistic Regression Evaluation\"\"\"\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lrClassifier = SklearnClassifier(LogisticRegression(max_iter=500))\n",
    "lrClassifier = crossValidation(lrClassifier, structuredLabledData)\n",
    "print(\"Logistic Regression Classifier: \")\n",
    "evaluationMeasures(lrClassifier, testingData)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88baa3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate Learning Curve\"\"\"\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "svmEstimator = SVC()\n",
    "\n",
    "trainSizes, trainScores, testScores, fitTimes, scoreTimes = learning_curve(svmEstimator, numericFeatureList, yTrain, cv=10, return_times=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d10dda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plot Learning Curve\"\"\"\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "axs[0].plot(trainSizes, numpy.mean(trainScores, axis=1), 'tab:blue', label = 'Train')\n",
    "axs[0].plot(trainSizes, numpy.mean(testScores, axis=1), 'tab:red', label = 'Validation')\n",
    "axs[0].set_xlabel('Experience')\n",
    "axs[0].set_ylabel('Score')\n",
    "axs[0].set_title('SVM Learning Curve')\n",
    "axs[0].legend(loc='lower right')\n",
    "axs[0].grid()\n",
    "\n",
    "axs[1].plot(trainSizes, numpy.mean(fitTimes, axis=1))\n",
    "axs[1].set_xlabel('Experience')\n",
    "axs[1].set_ylabel('Fit Time')\n",
    "axs[1].set_title('SVM Model Scailability')\n",
    "axs[1].grid()\n",
    "\n",
    "axs[2].plot(numpy.mean(fitTimes, axis=1), numpy.mean(testScores, axis=1))\n",
    "axs[2].set_xlabel('Fit Time')\n",
    "axs[2].set_ylabel('Score')\n",
    "axs[2].set_title('SVM Model Performance')\n",
    "axs[2].grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18418cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimising Model (Hyper Parameter Tuning - Kernel)\"\"\"\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svmClassifierOptamised = nltk.classify.SklearnClassifier(LinearSVC())\n",
    "svmClassifierOptamised = crossValidation(svmClassifierOptamised, structuredLabledData)\n",
    "print(\"Support Vector Machine Classifier: \")\n",
    "evaluationMeasures(svmClassifierOptamised, testingData)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b0cbad",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab044f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Final Pipeline\"\"\"\n",
    "\n",
    "#Data Acsess\n",
    "wordList = open('Data/complete-wordset.txt', 'r')\n",
    "wordSet = wordList.read().split()\n",
    "\n",
    "def classificationPipeline(document):\n",
    "    \"\"\"\n",
    "    Outlines the final classification pipeline process for an individual document.\n",
    "            \n",
    "    EXAMPLE: 'Apple's sales plumit in the latest quater' -> ['Apple's sales plumit in the latest quater', ['APPL'], 'negative']\n",
    "    \n",
    "    ARGS:\n",
    "        document (string) - A financial news headline.\n",
    "    \n",
    "    RETURNS:\n",
    "        [headline, securities, sentiment] - A list containing the headline (string), securities identified within the\n",
    "        headline (list of string values) and the classified sentiment (string).\n",
    "    \"\"\"\n",
    "    headline = document\n",
    "    document = preProcessing(document) #Pre-Processing\n",
    "    securities = namedEntityRecognition(headline)[1] #NER\n",
    "    features = (featureExtraction(wordSet, document)) #Feature Extraction\n",
    "    sentiment = svmClassifierOptamised.classify(features) #Classification\n",
    "    return [headline, securities, sentiment]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc5c680",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sentiment Processing\"\"\"\n",
    "\n",
    "#Data Acsess\n",
    "unclassifiedData = pandas.read_csv('Data/all_headlines.csv')\n",
    "unclassifiedData = cleanData(unclassifiedData)\n",
    "\n",
    "#Data Frame Construction\n",
    "classifiedData = pandas.DataFrame(data={'Document': [], 'Securities': [], 'Sentiment': [], 'Date': []})\n",
    "\n",
    "headlines = unclassifiedData[\"Headlines\"].to_list()\n",
    "\n",
    "start = time.time()\n",
    "for row in range(0, len(headlines)): #Headline Processing\n",
    "    entry = classificationPipeline(headlines[row])\n",
    "    entry.append(pandas.to_datetime(unclassifiedData[\"Date\"][row]))\n",
    "    classifiedData.loc[len(classifiedData)] = entry\n",
    "classifiedData.to_csv('Data/classified_data.csv')\n",
    "end = time.time()\n",
    "\n",
    "print(\"Runtime: \" + str(round((end - start), 2)) + 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f481bd6",
   "metadata": {},
   "source": [
    "## Result Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af25106",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sentiment Results - Distribution\"\"\"\n",
    "\n",
    "#Data Acsess \n",
    "classifiedData = pandas.read_csv('Data/classified_data.csv')\n",
    "\n",
    "#Sentiment Frequency\n",
    "sentiments = classifiedData[\"Sentiment\"].to_list()\n",
    "sentimentFrequency = Counter(sentiments)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 2))\n",
    "\n",
    "#Sentiment Frequency Plot\n",
    "labels = sentimentFrequency.keys()\n",
    "frequency = sentimentFrequency.values()\n",
    "axs[0].bar(labels, frequency, color=['grey', 'red', 'green'])\n",
    "axs[0].set_xlabel('Sentiment Polarity')\n",
    "axs[0].set_ylabel('Sentiment Occurance')\n",
    "axs[0].set_title('Sentiment Occurance From Classification')\n",
    "\n",
    "classifiedData = classifiedData[classifiedData['Sentiment'] != 'neutral'] #Objectivity Filtering\n",
    "sentiments = classifiedData[\"Sentiment\"].to_list()\n",
    "print(\"Number of Subjective Headlines: \" + str(len(sentiments)))\n",
    "\n",
    "#Security Frequency\n",
    "securities = classifiedData[\"Securities\"].to_list()\n",
    "securities = ' '.join(securities)\n",
    "securities = re.sub(r'[^a-zA-Z]', \" \", securities).split()\n",
    "securitiesFrequency = Counter(securities)\n",
    "securitiesFrequency.pop('GSPC')\n",
    "\n",
    "#Security Frequency Plot\n",
    "labels = securitiesFrequency.keys()\n",
    "frequency = securitiesFrequency.values()\n",
    "axs[1].bar(labels, frequency)\n",
    "axs[1].set_xlabel('Security')\n",
    "axs[1].set_ylabel('Security Occurance')\n",
    "axs[1].set_title('Security Occurance From Classification')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cb30b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Average Sentiment\"\"\"\n",
    "\n",
    "def calculateAverageSentiment(sentiments):\n",
    "    \"\"\"\n",
    "    Calculates and returns an average sentiment numeric value based on a list of\n",
    "    sentiments passed in as a parameter.\n",
    "            \n",
    "    EXAMPLE: ['positive', 'negative', 'negative', 'positive', 'positive'] -> 0.6\n",
    "    \n",
    "    ARGS:\n",
    "        sentiments (list of string values) - A list of the classified sentiments for individual securities. \n",
    "    \n",
    "    RETURNS:\n",
    "        average (float value) - The sentiment average numeric representation of the values passed in.\n",
    "    \"\"\"\n",
    "    totalSum = 0\n",
    "    for sentiment in sentiments:\n",
    "        if sentiment == 'positive': #Value Assignment\n",
    "            totalSum += 1\n",
    "    average = round(totalSum/len(sentiments), 3) #Average Calculation\n",
    "    return average\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff8ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Data Normalization\"\"\"\n",
    "\n",
    "def normalizeMarketData(marketData):\n",
    "    \"\"\"\n",
    "    Takes a list of close price values and returns them normalized into a range between 0 and 1.\n",
    "            \n",
    "    EXAMPLE: [10, 6, 8.5, 7, 2, 3] -> [1, 0.6, 0.85, 0.7, 0.2, 0.3]\n",
    "    \n",
    "    ARGS:\n",
    "        marketData (list of float values) - A list of stock close price values. \n",
    "    \n",
    "    RETURNS:\n",
    "        normalizedPriceList (list of float values) - A list of stock close price values normalized between 0 and 1.\n",
    "    \"\"\"\n",
    "    marketData = (marketData - numpy.min(marketData)) / (numpy.max(marketData) - numpy.min(marketData)) #Normalize\n",
    "    normalizedPriceList = []\n",
    "    for price in marketData: #Cleaning List\n",
    "        normalizedPriceList.append(price)\n",
    "    return normalizedPriceList\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17a5877",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Time Series (Sentiment and Price)\"\"\"\n",
    "\n",
    "def timeSeries(security):\n",
    "    \"\"\"\n",
    "    Generates a Time Series Plot for an individual security and its headline sentiment value.\n",
    "    \n",
    "    ARGS:\n",
    "        security (String) - Security Ticker.\n",
    "    \n",
    "    RETURNS:\n",
    "        monthlySentiment (list of float values) - List of average monthly sentiment values for an individual security.\n",
    "        monthlyPrice (list of float values) -  List of monthly normalized price values for an individual security.\n",
    "    \"\"\"\n",
    "    marketData = getMarketData(security)\n",
    "    normalizedPriceList = normalizeMarketData(marketData)\n",
    "    monthlyPrice = normalizedPriceList[::21] #Monthly Close Price\n",
    "    #Monthly Sentiment Results for Security\n",
    "    monthlySentimentData = classifiedData.loc[classifiedData['Securities'] == \"['\" + security + \"']\"]['Sentiment'].groupby([pandas.to_datetime(headlineData[\"Date\"]).dt.year, pandas.to_datetime(headlineData[\"Date\"]).dt.month])\n",
    "    missingValues = len(monthlyPrice) - len(monthlySentimentData)\n",
    "    monthlySentiment = []\n",
    "    for sentiment in monthlySentimentData:\n",
    "        sentimentList = sentiment[1].to_list()\n",
    "        sentimentAverage = calculateAverageSentiment(sentimentList)\n",
    "        monthlySentiment.append(sentimentAverage)\n",
    "    insertPlace = len(monthlyPrice) // missingValues\n",
    "    for x in range(0, missingValues):\n",
    "        monthlySentiment.insert(insertPlace, 0.5)\n",
    "        insertPlace += insertPlace\n",
    "    #Months with Insufficient Data\n",
    "    monthlySentiment = monthlySentiment[2:]\n",
    "    monthlyPrice = monthlyPrice[2:]\n",
    "    return monthlySentiment, monthlyPrice\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3232d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Scatter Plot (Sentiment Against Price Change)\"\"\"\n",
    "\n",
    "def scatterPlot(security):\n",
    "    \"\"\"\n",
    "    Generates a Scatter Plot for an individual security and its headline sentiment value.\n",
    "    \n",
    "    ARGS: \n",
    "        security (String) - Security Ticker.\n",
    "    \n",
    "    RETURNS:\n",
    "        dailySentimentValues (list of float values) - List of average daily sentiment values for an individual security.\n",
    "        dailyPriceChangeValues (list of float values) - List of daily normalized price change values for an individual security.\n",
    "    \"\"\"\n",
    "    marketData = getMarketData(security) #Collect Data\n",
    "    dailyPriceChange = numpy.diff(marketData) #Calculate Change\n",
    "    dailyPriceChange = normalizeMarketData(dailyPriceChange) #Normalize Price Values\n",
    "    #Daily Sentiment Results for Security\n",
    "    dailySentimentData = classifiedData.loc[classifiedData['Securities'] == \"['\" + security + \"']\"]['Sentiment'].groupby(classifiedData[\"Date\"])\n",
    "    #Ignore non-trading days\n",
    "    marketData = marketData.reset_index()\n",
    "    tradingDates = marketData['Date'].to_list()\n",
    "    formattedTradingDates = []\n",
    "    for date in tradingDates:\n",
    "        date = str(date.date())\n",
    "        formattedTradingDates.append(date)\n",
    "    dailySentiment = []\n",
    "    for date in formattedTradingDates:\n",
    "        dailySentiment.append('NA')\n",
    "    for sentiment in dailySentimentData:\n",
    "        if sentiment[0] in formattedTradingDates:\n",
    "            sentimentList = sentiment[1].to_list()\n",
    "            sentimentAverage = calculateAverageSentiment(sentimentList)\n",
    "            dailySentiment[formattedTradingDates.index(sentiment[0])] = sentimentAverage\n",
    "    dailySentimentValues, dailyPriceChangeValues = [], []\n",
    "    for x in range(0, len(dailySentiment) - 1):\n",
    "        if dailySentiment[x] != 'NA':\n",
    "            dailySentimentValues.append(dailySentiment[x])\n",
    "            dailyPriceChangeValues.append(dailyPriceChange[x])\n",
    "    return dailySentimentValues, dailyPriceChangeValues\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd58d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Visualize Results\"\"\"\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def securityResults(security):\n",
    "    \"\"\"\n",
    "    Generates an overview of the results for an individual security and its headline sentiment. \n",
    "    Visualisations include a time series plot and scatter plot. Metrics include a correlation \n",
    "    coefficient, test statistic and p-value.\n",
    "    \n",
    "    ARGS: \n",
    "        security (String) - Security Ticker.\n",
    "    \n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(16, 2))\n",
    "    #Plot Time Series \n",
    "    reflectGradient, intercept, rValue, pValue, stdError = stats.linregress(timeSeries(security)[0][:28], timeSeries(security)[1][:28])\n",
    "    axs[0].plot(timeSeries(security)[0], 'tab:blue', label='Headline Sentiment')\n",
    "    axs[0].plot(timeSeries(security)[1], 'tab:red', label= str(security + ' Price'))\n",
    "    axs[0].set_title('Monthly ' + security + ' Headline Sentiment and Market Price')\n",
    "    axs[0].set_xlabel('(Febuary 2018 - July 2020)')\n",
    "    axs[0].legend(loc='upper left')\n",
    "    #Plot Scatter Graph Change\n",
    "    axs[1].scatter(scatterPlot(security)[0], scatterPlot(security)[1], c='black', marker='x')\n",
    "    directGradient, intercept, rValue, pValue, stdError = stats.linregress(scatterPlot(security)[0], scatterPlot(security)[1])\n",
    "    x1 = numpy.linspace(numpy.min(scatterPlot(security)[0]), numpy.max(scatterPlot(security)[0]) ,500)\n",
    "    y1 = directGradient * x1 + intercept\n",
    "    axs[1].plot(x1, y1,'-r')\n",
    "    axs[1].set_title('Daily ' + security + ' Headline Sentiment Against Market Price Change')\n",
    "    axs[1].set_xlabel('Daily ' + security + ' Price Change (USD)')\n",
    "    axs[1].set_ylabel('Daily Sentiment Change')\n",
    "    #Statistical Analysis\n",
    "    directionStatistics = stats.ttest_ind(scatterPlot(security)[0], scatterPlot(security)[1])\n",
    "    reflectionStatistics = stats.ttest_ind(timeSeries(security)[0][:28], timeSeries(security)[1][:28])\n",
    "    print('Direction Correlation: ' + str(round(directGradient, 2)))\n",
    "    print('Direction T-Value: ' + str(round(directionStatistics[0], 2)))\n",
    "    print('Direction P-Value: ' + str(round(directionStatistics[1], 2)))\n",
    "    print('Reflection Correlation: ' + str(round(reflectGradient, 2)))\n",
    "    print('Reflection T-Value: ' + str(round(reflectionStatistics[0], 2)))\n",
    "    print('Reflection P-Value: ' + str(round(reflectionStatistics[1], 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc9df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"S&P 500 Results\"\"\"\n",
    "\n",
    "securityResults('^GSPC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db292c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Apple Results\"\"\"\n",
    "\n",
    "securityResults('AAPL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Google Results\"\"\"\n",
    "\n",
    "securityResults('WMT')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2fdbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Amazon Results\"\"\"\n",
    "\n",
    "securityResults('AMZN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Google Results\"\"\"\n",
    "\n",
    "securityResults('GOOGL')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bc46b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Microsoft Results\"\"\"\n",
    "\n",
    "securityResults('MSFT')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13875a7d",
   "metadata": {},
   "source": [
    "Author: Harvey Allen (1926159)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
